{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "#Data Mining [EN.550.436]\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "**Class 6** - Sept 15, 2015\n",
    "\n",
    "- Principal Component Analysis\n",
    "- Lagrange multipliers\n",
    "- Explained variance \n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "- Anonymous voting on Doodle\n",
    "\n",
    "> Use any alias you like at\n",
    "\n",
    ">    http://goo.gl/C8Vz9c\n",
    "\n",
    "- Be honest to help me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- A linear combination of known $\\phi_k(\\cdot)$ functions (basis functions)\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\sum_{k=1}^K \\beta_k\\, \\phi_k(x) $\n",
    "\n",
    "> It's a dot product\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\boldsymbol\\beta^T \\boldsymbol\\phi(x)$ \n",
    "\n",
    ">with $\\boldsymbol{\\beta}=(\\beta_1,\\dots,\\beta_K)^T$\n",
    "\n",
    "> or\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\boldsymbol{}X\\boldsymbol\\beta$\n",
    "\n",
    "> where $X_{ik} = \\phi_k(x_i)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method of Least Squares\n",
    "\n",
    "- At the optimum\n",
    "\n",
    ">$\\displaystyle \\hat\\beta = (X^T X)^{-1} X^T y $\n",
    "\n",
    "- Hat matrix\n",
    "\n",
    ">$\\hat{y} = X\\hat\\beta = H y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combinations\n",
    "\n",
    "- Coefficients mix a given set of basis vectors, functions, images, shapes, ...\n",
    "\n",
    "\n",
    "> Fourier series\n",
    "\n",
    "<img src=files/Periodic_identity_function.gif width=400> \n",
    "<!--<img src=https://upload.wikimedia.org/wikipedia/commons/e/e8/Periodic_identity_function.gif width=400> -->\n",
    "\n",
    "> Discete Cosine Transform (JPEG) \n",
    "\n",
    "<img src=files/DCT_basis_thumb.gif width=200>\n",
    "<!--<img src=http://www.digitude.net/blog/wp-content/uploads/2010/07/DCT_basis_thumb.gif width=200>-->\n",
    "\n",
    "> Spherical Harmonics\n",
    "\n",
    "<img src=files/Spherical_Harmonics.png>\n",
    "<!--<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Spherical_Harmonics.png/300px-Spherical_Harmonics.png>-->\n",
    "\n",
    "\n",
    "- What is a good basis like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Principal Component Analysis</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Learning\n",
    "\n",
    ">|                | Supervised     |         Unsupervised     |\n",
    " |:---------------|:--------------:|:------------------------:|\n",
    " | **Discrete**   | Classification | Clustering               |   \n",
    " | **Continuous** | Regression     | Dimensionality Reduction |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=files/800px-GaussianScatterPCA.png width=300 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions of Maximum Variance\n",
    "\n",
    "- Let $X\\in\\mathbb{R}^N$ be a continuous random variable with 0 mean and covariance matrix $C$. What is the direction of maximum variance?\n",
    "\n",
    "> For any vector $a\\in\\mathbb{R}^N$ \n",
    "\n",
    "> $\\displaystyle \\mathbb{Var}[a^T X] = \\mathbb{E}\\left[(a^T X)(X^T a)\\right] = \\mathbb{E}\\left[a^T(XX^T)\\,a\\right] = a^T C\\,a$\n",
    "\n",
    "> We have to maximize this such that $a^2\\!=\\!1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Optimization\n",
    "\n",
    "- **Lagrange multiplier**: extra term with new parameter $\\lambda$\n",
    "\n",
    "> $\\displaystyle  \\min_{a\\in{}\\mathbb{R}^N} \\left\\{a^T C\\,a - \\lambda\\,(a^2\\!-\\!1)\\right\\}$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial\\lambda} \\rightarrow\\ \\  a^2\\!-\\!1 = 0\\ \\ $  (duh!)\n",
    "\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial a_k} \\rightarrow\\ \\  $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With indices\n",
    "\n",
    "\n",
    "> $\\displaystyle  \\sum_{i,j} a_i C_{ij} a_j - \\lambda\\,\\left(\\sum_i a_i^2 - 1\\right)$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\frac{\\partial a_i}{\\partial a_k} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\frac{\\partial a_j}{\\partial a_k} - 2\\lambda\\,\\left(\\sum_i a_i \\frac{\\partial a_i}{\\partial a_k}\\right) = 0$ \n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\delta_{ik} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\delta_{jk} - 2\\lambda\\,\\left(\\sum_i a_i \\delta_{ik}\\right) = 0$ \n",
    "\n",
    "> $\\displaystyle \\sum_{j} C_{kj} a_j + \\sum_{i} a_i C_{ik}  - 2\\lambda\\,a_k = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And back again...\n",
    "\n",
    "- With vectors and matrices\n",
    "\n",
    "> $\\displaystyle  Ca + C^Ta - 2\\lambda a = 0$\n",
    "\n",
    "> but $C$ is symmetric \n",
    "\n",
    "> $\\displaystyle  Ca = \\lambda a $\n",
    "\n",
    "- Eigenproblem !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- The value of maximum variance is\n",
    "\n",
    "> $\\displaystyle  a^TCa = a^T \\lambda a = \\lambda a^Ta = \\lambda$\n",
    "\n",
    "> the largest eigenvalue $\\lambda_1$\n",
    "\n",
    "- The direction of maximum variance is the corresponding eigenvector $a_1$\n",
    "\n",
    "> $\\displaystyle  Ca_1 = \\lambda_1 a_1 $\n",
    "\n",
    "- This is the **1st Principal Component** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Principal Component\n",
    "\n",
    "- Direction of largest variance uncorrelated to 1st PC\n",
    "\n",
    "> $\\displaystyle  \\min_{a\\in{}\\mathbb{R}^N} \\left\\{a^T C\\,a - \\lambda\\,(a^2\\!-\\!1) - \\lambda'(a^T C\\,a_1) \\right\\}$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle 2Ca - 2\\lambda{}a-\\lambda'Ca_1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Multiply by $a_1^T\\cdot$\n",
    "\n",
    "> $\\displaystyle 2a_1^TCa - 2a_1^T\\lambda{}a-a_1^T\\lambda'Ca_1 = 0$\n",
    "\n",
    "> $\\displaystyle 0 - 0 - \\lambda'\\lambda_1 = 0 \\ \\ \\rightarrow\\ \\  \\lambda'=0$\n",
    "\n",
    "- Eigenproblem \n",
    "\n",
    "> $\\displaystyle  Ca = \\lambda a $\n",
    "\n",
    "- Solution $\\lambda_2$ and $a_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA \n",
    "\n",
    "- In general\n",
    "\n",
    "> Let $\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_N$ be the eigenvalues of $C$ and $\\hat{e}_1,\\dots,\\hat{e}_N$ the corresponding eigenvectors\n",
    "\n",
    "> $\\displaystyle  C = \\sum_{k=1}^N\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) $\n",
    "\n",
    "> With diagonal $\\Lambda$ matrix of the eigenvalues and an $E$ matrix of $[\\hat{e}_1, \\dots, \\hat{e}_N]$\n",
    "\n",
    "> $\\displaystyle  C = E\\ \\Lambda\\ E^T$\n",
    "\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> If keeping only $K<N$ eigenvectors, the best approximation is taking the first $K$ PCs\n",
    "\n",
    "> $\\displaystyle  C \\approx \\sum_{k=1}^K\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) =  E_K\\Lambda_KE_K^T$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Adding Variances\n",
    "\n",
    "- If $X$ and $Y$ random variables are independent and $Z=X\\pm{}Y$, then \n",
    "\n",
    "> $\\mathbb{Var}[Z]=\\mathbb{Var}[X]+\\mathbb{Var}[Y]$\n",
    "\n",
    "\n",
    "- Proof\n",
    "\n",
    "> $\\mathbb{Var}[Z]=\\mathbb{E}\\big[(Z-\\mu_Z)^2\\big] $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[Z^2\\big] - 2\\mathbb{E}[Z]\\mu_Z + \\mu_{z}^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[Z^2\\big] - \\mu_{z}^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[(X\\pm{}Y)^2\\big] - \\left(\\mu_{X\\pm{}Y}\\right)^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[(X\\pm{}Y)^2\\big] - \\left(\\mu_{X}\\pm{}\\mu_Y\\right)^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[X^2\\pm{}2XY+Y^2\\big] - \\left(\\mu_{X}^2\\pm{}2\\mu_X\\mu_Y+\\mu_Y^2\\right)$\n",
    "\n",
    "> $\\ \\ = \\mathbb{E}\\big[X^2\\big] \\pm 2\\,\\mathbb{E}\\big[XY\\big] + \\mathbb{E}\\big[Y^2\\big] - \\left(\\mu_{X}^2\\pm{}2\\mu_X\\mu_Y+\\mu_Y^2\\right)$\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[X^2\\big]-\\mu_X^2 +\\mathbb{E}\\big[Y^2\\big]-\\mu_Y^2 \\pm 2\\left(\\mathbb{E}\\big[XY\\big] -\\mu_X\\mu_Y\\right) $\n",
    "\n",
    "\n",
    "> $\\ \\ =\\mathbb{Var}[X]+\\mathbb{Var}[Y]\\pm 0 $\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Coordiante System\n",
    "\n",
    "- The $E$ matrix of eigenvectors is a rotation, $E\\,E^T = I$\n",
    "\n",
    "> $\\displaystyle  Z = E^T\\, X $\n",
    "\n",
    "\n",
    "- A truncated set of eigenvectors $E_K$ defines a projection\n",
    "\n",
    "> $\\displaystyle  Z_K = E_K^T\\, X $\n",
    "\n",
    "> and\n",
    "\n",
    "> $\\displaystyle  X_K = E_K E_K^T\\, X = P_K\\,X $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Projections\n",
    "\n",
    "- If the square of a matrix is equal to itself\n",
    "\n",
    "> $\\displaystyle  P^2 = P $\n",
    "\n",
    "- For example, projecting on the $\\hat{e}$ unit vector\n",
    "\n",
    "<img src=files/Y7Gx8.png align=right width=250>\n",
    "\n",
    "> Scalar times vector\n",
    "\n",
    "> $\\displaystyle  r' = \\hat{e}\\left(\\hat{e}^T r\\right) = \\hat{e} \\beta_r$\n",
    "\n",
    "> Or  projection of vector $r$\n",
    "\n",
    "> $\\displaystyle  r' = \\left(\\hat{e}\\,\\hat{e}^T\\right)r = P\\,r$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> $\\displaystyle  C \\approx C_K = \\sum_{k=1}^K\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) = \\sum_{k=1}^K\\ \\lambda_k\\,P_k$\n",
    "\n",
    "- And the remaining eigenvectors span the subspace with the least variance\n",
    "\n",
    "> $\\displaystyle  C - C_K = %\\sum_{l=K+1}^N\\ \\lambda_l\\left(\\hat{e}_l\\,\\hat{e}_l^T\\right) =\n",
    "\\sum_{l=K+1}^N\\ \\lambda_l\\,P_l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples\n",
    "\n",
    "- Set of $N$-vectors with expectation value (or average, cf. Bessel correction) 0 arranged in $X=\\left[x_1, x_2, \\dots\\right]$ \n",
    "\n",
    "> Sample covariance matrix is\n",
    "\n",
    ">$\\displaystyle C \\propto X X^T = \\sum_i x_i x_i^T$\n",
    "\n",
    "- Singular Value Decomposition (SVD)\n",
    "\n",
    ">$\\displaystyle X = U W V^T$\n",
    "\n",
    "> where $U^TU=I$, $W$ is diagonal, and $V^TV=I$\n",
    "\n",
    "- Hence\n",
    "\n",
    ">$\\displaystyle C \\propto UWV^T\\ VWU^T = U W^2 U^T$\n",
    "\n",
    "> So if $C=E\\Lambda E^T$, then $E = U$ and $ \\Lambda \\propto W^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['norm']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample from Bivariate Normal \n",
    "\n",
    "- See previous lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "[[ 1.67010483]\n",
      " [ 3.43703007]]\n",
      "Covariance\n",
      "[[ 13.04682737  11.34309787]\n",
      " [ 11.34309787  11.28168218]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAE4CAYAAAAuFPo7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSB5BCCB5BCGA5BGEAJL3/5HlTcIiK050AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1074540d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "# generate many 2D (column) vectors\n",
    "S = norm.rvs(0,1,(2,20))\n",
    "S[0,:] *= 4  # scale axis 0\n",
    "f = +pi/4    # rotate by 45 degrees\n",
    "R = array([[cos(f), -sin(f)],\n",
    "           [sin(f),  cos(f)]]) \n",
    "X = R.dot(S)\n",
    "X += np.array([[1],[3]]) # shift\n",
    "\n",
    "figure(figsize=(5,5)); xlim(-15,15); ylim(-15,15);\n",
    "plot(X[0,:],X[1,:],'o',alpha=0.9)\n",
    "\n",
    "# subtract sample mean\n",
    "avg = mean(X, axis=1).reshape(X[:,1].size,1)\n",
    "X -= avg\n",
    "# sample covariance matrix\n",
    "C = X.dot(X.T) / (X[0,:].size-1) \n",
    "print \"Average\\n\", avg\n",
    "print \"Covariance\\n\", C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.73402063, -0.67912717],\n",
       "        [ 0.67912717,  0.73402063]]), array([ 23.541636  ,   0.78687355]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, E = np.linalg.eig(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.73402063, -0.67912717],\n",
       "        [-0.67912717,  0.73402063]]), array([ 23.541636  ,   0.78687355]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E, L, E_same = np.linalg.svd(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -1.66533454e-16],\n",
       "       [ -1.66533454e-16,   1.00000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.dot(E.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose( E.T, np.linalg.inv(E) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.73402063, -0.67912717],\n",
       "        [-0.67912717,  0.73402063]]), array([ 23.541636  ,   0.78687355]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, W, V = np.linalg.svd(X)\n",
    "U, W**2 / (X[0,:].size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.allclose( U.dot(U.T), np.eye(U[:,0].size) ), \n",
    "  np.allclose( V.dot(V.T), np.eye(V[:,0].size) )  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.73402063,  0.67912717],\n",
       "        [-0.67912717, -0.73402063]]), array([ 22.3645542 ,   0.74752987]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=X[:,0].size)\n",
    "pca.fit(X.T) # different convention !!!\n",
    "# pca.transform(X.T)\n",
    "pca.components_.T, pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
